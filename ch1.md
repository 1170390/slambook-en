## Preface

### What is this book about?

This is a book that introduces visual SLAM, and it is probably the first Chinese book focused on visual SLAM.

So, what is SLAM?

SLAM stands for **S**imultaneous **L**ocalization **a**nd **M**apping. It usually refers to a robot or a moving rigid body, equipped with a specific **sensor**, estimates its own **motion** and builds a **model** (some kinds of description) of the surrounding environment, without a priori information [Davison2007]. If the sensor here is mainly for the camera, it is called "visual SLAM".

Visual SLAM is the subject of this book. We deliberately put a lot of definitions into a sentence, so that readers may have a more clear concept. First of all, SLAM aims to solve the "positioning" and "map building" issues at the same time. In other words, it is a problem of how to estimate the location of the sensor itself, while estimate the establishment of the environment. So how to solve it? This requires a good understanding about the sensor information. The sensor can observe the external world in a certain form, but the specific approach of such observations is different. And, why is this question worthwhile to spend the whole contents of a book discussing it? Because it is difficult, especially if we want to make SLAM **in real time** and **without any prior knowledge**. When we talk about visual SLAM, what we have to do is to estimate the trajectory and map based on a set of continuous images (they form a video).

This seems to be a very intuitive question. When we humans go into a strange environment, aren't we doing exactly the same thing? So, can we write programs and make the computer do it? 

During the birth of computer vision field, people imagined that one day the computer would be like humans, watching and observing the world, in order to understand the surrounding objects. The ability to explore the unknown areas is  a wonderful and romantic dream, attracting numerous scientific researchers to struggle with this problem day and night [Hartyley2003]. We thought that this would not to be that difficult, but actually, the progress is not smooth as expected. Flowers, trees, insects, birds and animals, look so different in  computers: they are only the one by one numbers of a matrix. It is as difficult for the computer to understand the contents of the image as it is for we humans to understand those numbers. We don't know how to understand the image, nor do we know how to understand the computer. So, after decades of confusion,  we finally can see a little sign of success - by Artificial Intelligence (AI) and Machine Learning (ML) technology, which gradually allows the computers to identify the objects, faces, voices, texts, although in a way (probabilistically) that is still so different from ours. On the other hand, after nearly two years of developments in SLAM, our cameras begin to realize their position and find themselves in the motion and, although there is still a huge gap between the computers and our humans, researchers have successfully built a variety of real-time SLAM systems. Some can quickly track their own location, and the other can even do three-dimensional reconstruction in real-time.

This is really difficult, but we have made a lot of progress. What's even more exciting is that, in recent years, we have seen the emergence of a large number of SLAM-related applications. The location of sensor is really useful in many areas: indoor sweeping machines and mobile robots need to locate; automatic driving cars in the roads need to locate; UAVs in the air need to locate; Virtual Reality (VR) and Augmented Reality (AR) sensors also need to locate. SLAM is so important. Without it, the sweeping machine cannot move in the room autonomously, only wandering blindly; domestic robots can not follow the instructions to reach a certain room accurately; Virtual Reality will always be fixed on the seat. If all these new things could not appear in real life, it would be so regrettable.

Today's researchers and developers are increasingly aware of the importance of SLAM technology.  SLAM has nearly 30 years of research history, and has also been a hot topic both in robot and computer vision community . Since the 21st century, visual SLAM technology has undergone a significant change and breakthrough in theory and practice, and is gradually going out from the laboratory into the real-world application. At the same time, we regretfully find that, at least in Chinese, SLAM-related papers and books are still very scarce, making many beginners interested in this area cannot get into the door. Although the theoretical framework of SLAM is basically matured, to write a whole SLAM system is still very complex which requires a high technical level. Researchers who have just entered SLAM area have to spend a long time learning a lot of basic and scattered knowledge, and often have to go through many detours to get close to the core of SLAM technology.

This book systematically introduces the visual SLAM technology. We hope that it will (in part) fill the gaps. We will detail SLAM's theoretical background, system architecture, and the mainstream of the various modules. At the same time, we attach great importance onto practice: all the important algorithms introduced in this book will have runnable code that can be tested by yourself, in order to deepen the reader's understanding. Visual SLAM, after all, is a technology that is very closely related to practice. Although the mathematical theory is beautiful, if you cannot convert it into a program, it is still like a castle in the air, which means no practical significance. We believe that the practice is true knowledge, and practice brings true love. Only after actually calculating the various algorithms, you can really understand SLAM, and fall in love with research.

Since its inception in 1986 [Smith1986], SLAM has been a hot topic in the field of robotics. It is very difficult and unnecessary to make a complete description of all the algorithms and their variants in the history of SLAM. This book will introduce the background knowledge, such as projective geometry, computer vision, state estimation theory, Lie Group and Lie algebra, etc., and on top of these background knowledge, give the trunk of the SLAM tree, and omit those strange and complex leaves. We think this is effective. If the reader can master the essence of the trunk, then they can naturally have the ability to explore those edges and the details of the complex frontier knowledge. So our aim is to allow SLAM beginners to quickly grow into qualified researchers of this field by reading this book. On the other hand, even if you are already an experienced SLAM researcher, this book may also have some places where you feel unfamiliar and can give you new insights.

There are already some SLAM-related books likes "Probabilistic robotics" [Thrun2005], "Multiple View Geometry in Computer Vision" [Hartley2003], "State Estimation for Robotics: A Matrix-Lie-Group Approach"[Barfoot2017] and so on. They provide rich contents, comprehensive discussions and rigorous derivations, therefore being the most popular textbooks among SLAM researchers. However,  there are two important issues: First, the purpose of these books is to introduce the basic mathematical theory, while SLAM is only one of its applications. Therefore, they cannot be considered specifically focused on visual SLAM. Second, they place great emphasis on mathematical theory, but are not involved in programming, making the readers cannot actually apply the knowledge into practice. And we believe that one can talk about the real understanding of a problem after coding, debugging and adjusting the algorithms and parameters personally.

We will mention the history, theory, algorithms and status in SLAM, and divide a complete SLAM system into several modules: visual odometry, back-end optimization, map building, and loop closure detection. We will accompany the reader step by step to implement the core technology of these modules, to explore why they are effective, to under which situation they will be ill-conditioned, and to guide everyone to run the code in their own machines. You will be exposed to some of the necessary mathematical theory and many programming knowledge, and will use Eigen, OpenCV, PCL, g2o, Ceres and other libraries, in order to master their use in the Linux operating system.

Well, enough talking, wish you a pleasant trip!

### How to use this book?

This book is titled "14 Lectures on Visual SLAM". As the name suggests, we will organize the contents into "lectures" like what we listen in school. Each lecture corresponds to a fixed topic, which will have some "theoretical parts" and "practice parts". Usually we put the theoretical parts in front of a chapter, and then the practice part. In the former, we will introduce the mathematical knowledge necessary to understand the algorithm, and most of the time in the narrative way, rather than the use of mathematical textbooks like "definition, theorem, inference" approach, because we think this is easier to understand , although sometimes is less rigorous. In practice part we will give the codes and  discuss the meaning of the various parts of a program and show some experimental results. So, when you see the chapter with the word "practice" in the title, you should open the computer and write the code with us happily.

Basically we have two parts in our book: The first part is about basic math knowledges, which contains:

1. Lecture 1 is preface (just this lecture). It talks about the contents and structure of this book .
2. Lecture 2 is an overview of a SLAM system. It describes each module of a SLAM system and explains what and how they work. The practice section introduces the basic C++ programing in Linux environment and the use of the IDE.
3. Lecture 3 introduce the rigid body motion in 3D space. You will learn the knowledge about rotation matrix, quaternion, Euler angle, and practice them in Eigen.
4. Lecture 4 is about Lie group and Lie algebra. It doesn't matter if you have not heard about them. You will learn the basics of  Lie group, and then manipulate them through Sophus.
5. Lecture 5 introduces the pinhole camera model and the expression of images in the computer. You will use OpenCV to retrieve the camera's internal and external parameters, and then make a point cloud from depth information through PCL (Point Cloud Library). 
6. Lecture 6 introduces the nonlinear optimization, including the state estimation, least squares and gradient descent methods like Gauss-Newton and Levenburg-Marquardt. You will solve a curve fitting problem through Ceres and g2o library.

From lecture 7, we will go into SLAM algorithms, starting from Visual Odometry (VO) and then map building problems: 

7. Lecture 7 talks about feature based visual odometry, which is currently the mainstream in VO. The content includes the feature extraction and matching, the calculation of epipolar geometry, PnP, ICP, Bundle Adjustment (BA) and so on.  You will run these methods either by calling OpenCV's functions or by constructing you own optimization problem in Ceres and g2o.
8. Lecture 8 is about the direct method (or intensity-based) in VO. You will learn the principle of optical flow and direct method, and then use g2o to achieve a simple RGB-D direct method based VO, because the optimization in most direct VO algirhtms is rather complecated.
9. Lecture 9 is a practice chapter for VO. You will build a visual odometer framework by yourself, integrate the previously learned knowledge, and solve some problems such as how to manage the frame and map point, how to choose key frames and how to control the optimization.
10. Lecture 10 is about back-end optimization. We will discussion the Bundle Adjustment in detail, and show the relationship between the sparse structure of BA and its corresponding graph model. You will separately use Ceres and g2o to solve the same BA problem.
11. Lecture 11 mainly talks about the pose graph in the back-end optimization. Pose graph is a more compact form of BA which marginalizes all the map points into the constraints between keyframes. You will use g2o and gtsam to optimize a pose graph.
12. Lecture 12 is for the loop closure detection, mainly talking about Bag-of-Word (BoW) based method. You will use dbow3 to train a dictionary from images and check the loops in images. 
13. Lecture 13 is about map building. We will discuss how to estimate the depth of feature points in monocular SLAM  (and show why they are unreliable). Compared with monocular depth estimation, build a dense map with RGB-D cameras is much easier. You will write the epipolar line search and patch matching programs to estimate the depth from monocular images, and then build a point cloud map and octagonal tree map from RGB-D data.
14. Lecture 14 introduces the current open source SLAM projects and the future development direction. I believe that after reading the previous chapters, you can understand their approaches easily, and will be able to  achieve your own new ideas.

Finally, if you don't understand what I'm talking about at all, then congratulations! This book is just for you! Come on and fight!

### Style

### Acknowledgements

### Practice

### References

- [Davison2007]. A. Davison, I. Reid, N. Molton, and O. Stasse, “Monoslam: Real-time single camera SLAM,” IEEETransactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 6, pp. 1052–1067, 2007.
- [Hartley2003]. R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision. Cambridge universitypress, 2003.
- [Smith1986]. R. C. Smith and P. Cheeseman, “On the representation and estimation of spatial uncertainty,” In-ternational Journal of Robotics Research, vol. 5, no. 4, pp. 56–68, 1986.
- [Thrun2005]. S. Thrun, W. Burgard, and D. Fox, Probabilistic robotics. MIT Press, 2005.
- T. Barfoot, “State estimation for robotics: A matrix lie group approach,”, Cambridge Press, 2016.
