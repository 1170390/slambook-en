## Preface

### What is this book about?

This is a book that introduces visual SLAM, and it is probably the first Chinese book focused on visual SLAM.

So, what is SLAM?

SLAM stands for **S**imultaneous **L**ocalization **a**nd **M**apping. It usually refers to a robot or a moving rigid body, equipped with a specific **sensor**, estimates its own **motion** and builds a **model** (some kinds of description) of the surrounding environment, without a priori information [Davison2007]. If the sensor here is mainly a camera, it is called "visual SLAM".

Visual SLAM is the subject of this book. We deliberately put a lot of definitions into a sentence, so that readers may have a more clear concept. First of all, SLAM aims at solving the "positioning" and "map building" issues at the same time. In other words, it is a problem of how to estimate the location of the sensor itself, while estimating the model of the environment. So how to solve it? This requires a good understanding of the sensor information. The sensor can observe the external world in a certain form, but the specific approach for such observations could be different. And, why is this question worthwhile spending the a whole book to discuss? Simply because it is difficult, especially if we want to make SLAM **real time** and **without any prior knowledge**. When we talk about visual SLAM, what we have to do is to estimate the trajectory and map based on a set of continuous images (they form a video).

This seems to be a very intuitive question. When we humans enter an unfamiliar environment, aren't we doing exactly the same thing? So, can we write programs and make the computer do it? 

At the birth of computer vision, people imagined that one day computers could act like humans, watching and observing the world, in order to understand the surrounding objects. The ability of exploring unknown areas is  a wonderful and romantic dream, attracting numerous researchers striving on this problem day and night [Hartyley2003]. We thought that this would not be that difficult, but actually, the progress is not as smooth as expected. Flowers, trees, insects, birds and animals, appear so different in  computers: they are just matrices composed of numbers. It is as difficult for computers to understand the contents of the images as it is for us human to understand those numbers. We didn't know how we understand images, nor do we know how to make computers understand them. So, after decades of struggling,  we finally started to see signs of success - through Artificial Intelligence (AI) and Machine Learning (ML) technologies, which gradually enable computers to identify objects, faces, voices, texts, although in a way (probabilistic modeling) that is still so different from us. On the other hand, after nearly three decades of development in SLAM, our cameras begin to realize their movement and understand their positions, although there is still a huge gap between computers and human, researchers have successfully built a variety of real-time SLAM systems. Some can efficiently track their own locations, and others can even do three-dimensional reconstruction in real-time.

This is really difficult, but we have made a lot of progress. What's even more exciting is that, in recent years, we have seen the emergence of a large number of SLAM-related applications. The sensor location is very useful in many areas: indoor sweeping machines and mobile robots, automatic driving cars, UAVs in the air, Virtual Reality (VR) and Augmented Reality (AR) applications. SLAM is so important. Without it, the sweeping machine cannot maneuver in a room autonomously, but wandering blindly; domestic robots can not follow the instructions to reach a certain room accurately; Virtual Reality will always be limited within a prepared space. If none of these innovations could appear in real life, it would be so regrettable.

Today's researchers and developers are increasingly aware of the importance of SLAM technology.  SLAM has nearly 30 years of research history, and has been a hot topic in both robotics and computer vision community . Since the 21st century, visual SLAM technology has undergone a significant change and breakthrough in theory and practice, and is gradually moving out from laboratories into real-world. At the same time, we regretfully find that, at least in the Chinese language, SLAM-related papers and books are still very scarce, making many beginners interested in this area not able to get started smoothly. Although the theoretical framework of SLAM has become basically mature, to write a complete SLAM system is still very complex and requires a high technical level. Researchers who have just entered the SLAM area have to spend a long time learning significant amount of scattered fundamental knowledge, and often have to go through quite a number of detours to get close to the core of the SLAM technology.

This book systematically introduces the visual SLAM technology. We hope that it will (in part) fill the current gaps. We will detail SLAM's theoretical background, system architecture, and the various mainstream modules. At the same time, we place great emphasis on practice: all the important algorithms introduced in this book will be provided with runnable code that can be tested by yourself, so that readers can reach a deeper understanding. Visual SLAM, after all, is a technology that is very closely related to application. Although the mathematical theory can be beautiful, if you are not able to convert it into lines of code, it will still be like a castle in the air, which has little practical significance. We believe that practice verifies correct knowledge, and practice tests real passion. Only after directly manipulating the implementation of the various algorithms, you can really understand SLAM, and claim that you have fallen in love with SLAM research.

Since its inception in 1986 [Smith1986], SLAM has been a hot topic in the field of robotics. It is very difficult and unnecessary to give a complete introduction to all the algorithms and their variants in the SLAM history. This book will be firstly introducing the background knowledge, such as projective geometry, computer vision, state estimation theory, Lie Group and Lie algebra, etc. On top of that, we will be showing the trunk of the SLAM tree, and omitting those complicated and oddly-shaped leaves. We think this will be effective. If the reader can master the essence of the trunk, they will naturally have the ability to explore the details of the complex research frontier. So our aim is to help SLAM beginners quickly grow into qualified researchers or developers in this field. On the other hand, even if you are already an experienced SLAM researcher, this book may still cover areas that you are unfamiliar with and may provide you with some new insights.

There have already been a few SLAM-related books around, such as "Probabilistic robotics" [Thrun2005], "Multiple View Geometry in Computer Vision" [Hartley2003], "State Estimation for Robotics: A Matrix-Lie-Group Approach"[Barfoot2017], etc. They provide rich contents, comprehensive discussions and rigorous derivations, therefore being the most popular textbooks among SLAM researchers. However,  they have two important issues: First, the purpose of these books is to introduce the fundamental mathematical theory, with SLAM being only one of its applications. Therefore, they cannot be considered as visual SLAM focused. Second, they place great emphasis on mathematical theory, but are relatively lacking in programming. This makes readers still difficult in applying the knowledge they learned from the books. And this is our believe: only after coding, debugging and adjusting algorithms and parameters personally, one can claim real understanding of a problem.

In this book, we will be introducing the history, theory, algorithms and research status in SLAM, and explaining a complete SLAM system by decomposing into several modules: visual odometry, back-end optimization, map construction, and loop closure detection. We will be accompanying readers step by step to implement the core algorithms of each module, to explore why they are effective, under what situations they are ill-conditioned, and to guide everyone through running the code on their own machines. You will be exposed to the necessary mathematical theory and programming knowledge, and will use libraries including Eigen, OpenCV, PCL, g2o, and Ceres, and master their use in the Linux operating system.

Well, enough talking, wish you a pleasant journey!

### How to use this book?

This book is entitled "14 Lectures on Visual SLAM". As the name suggests, we will organize the contents into "lectures" like we are learning in a classroom. Each lecture focuses on one specific topic, expanding in a logical order. Each chapter will include both "theoretical parts" and "practical parts", with theoretical parts usually coming first, followed by practical parts. We will introduce the mathematics essential to understand the algorithms, and most of the time in a narrative way, rather than in a "definition, theorem, inference" approach used in mathematical textbooks, because we think this is easier to understand, with a price of being less rigorous sometimes. In practical parts, we will provide code and discuss the meaning of the various parts, and demonstrate some experimental results. So, when you see chapters with the word "practice" in the title, you should turn on your computer and start to program with us, passionately.

The book can be divided into two parts: The first part will be mainly focused on fundamental math knowledge, which contains:

1. Lecture 1: preface (the one you are reading now), introducing the contents and structure of the book.
2. Lecture 2: an overview of a SLAM system. It describes each module of a SLAM system and explains what they do and how they work. The practice section introduces the basic C++ programing in Linux environment and the use of an IDE.
3. Lecture 3: rigid body motion in 3D space. You will learn knowledge about rotation matrices, quaternions, Euler angles, and practice them using the Eigen library.
4. Lecture 4: Lie group and Lie algebra. It doesn't matter if you have never heard about them. You will learn the basics of Lie group, and then manipulate them through Sophus.
5. Lecture 5: pinhole camera model and image expression in computer. You will use OpenCV to retrieve camera's intrinsic and extrinsic parameters, and then generate a point cloud from depth information through PCL (Point Cloud Library). 
6. Lecture 6: nonlinear optimization, including state estimation, least squares and gradient descent methods, e.g. Gauss-Newton and Levenburg-Marquardt. You will solve a curve fitting problem using the Ceres and g2o library.

From lecture 7, we will be talking about SLAM algorithms, starting with Visual Odometry (VO) and then map building problems: 

7. Lecture 7: feature based visual odometry, which is currently the mainstream in VO. Content includes feature extraction and matching, epipolar geometry calculation, Perspective-n-Point (PnP), Iterative Closest Point (ICP), Bundle Adjustment (BA), etc. You will run these algorithms either by calling OpenCV's functions or by constructing you own optimization problem in Ceres and g2o.
8. Lecture 8: direct (or intensity-based) method in VO. You will learn the principle of optical flow and direct method, and then use g2o to achieve a simple RGB-D direct method based VO. Note that the optimization in most direct VO algorithms will be more complicated.
9. Lecture 9: a practice chapter for VO. You will build a visual odometer framework by yourself, by integrating the previously learned knowledge, to solve some problems such as how to manage frames and map points, how to choose key frames and control optimization.
10. Lecture 10: back-end optimization. We will discuss Bundle Adjustment in detail, and show the relationship between the sparse structure of BA and its corresponding graph model. You will use Ceres and g2o separately to solve the same BA problem.
11. Lecture 11: pose graph in the back-end optimization. Pose graph is a more compact representation for BA which marginalizes all map points into constraints between keyframes. You will use g2o and gtsam to optimize a pose graph.
12. Lecture 12: loop closure detection, mainly Bag-of-Word (BoW) based method. You will use dbow3 to train a dictionary from images and detect loops in images. 
13. Lecture 13: map building. We will discuss how to estimate the depth of feature points in monocular SLAM  (and show why they are unreliable). Compared with monocular depth estimation, building a dense map with RGB-D cameras is much easier. You will write programs for epipolar line search and patch matching to estimate depth from monocular images, and then build a point cloud map and octagonal tree map from RGB-D data.
14. Lecture 14: current open source SLAM projects and future development direction. I believe that after reading the previous chapters, you will understand other people's approaches easily, and will be able to achieve new ideas of your own.

Finally, if you don't understand what I'm talking about at all, then congratulations! This book is right for you! Come on and fight!

### Style

### Acknowledgements

### Practice

### References

- [Davison2007]. A. Davison, I. Reid, N. Molton, and O. Stasse, “Monoslam: Real-time single camera SLAM,” IEEETransactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 6, pp. 1052–1067, 2007.
- [Hartley2003]. R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision. Cambridge universitypress, 2003.
- [Smith1986]. R. C. Smith and P. Cheeseman, “On the representation and estimation of spatial uncertainty,” In-ternational Journal of Robotics Research, vol. 5, no. 4, pp. 56–68, 1986.
- [Thrun2005]. S. Thrun, W. Burgard, and D. Fox, Probabilistic robotics. MIT Press, 2005.
- T. Barfoot, “State estimation for robotics: A matrix lie group approach,”, Cambridge Press, 2016.
